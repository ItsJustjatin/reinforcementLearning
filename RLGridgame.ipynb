{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, size, obstacles=[], random_obstacles=False):\n",
        "        self.size = size\n",
        "        self.grid = np.zeros((size, size))  # Initialize grid with all zeros\n",
        "        self.agent_position = (0, 0)  # Agent starts at top-left corner\n",
        "        self.goal_position = (size-1, size-1)  # Goal is at bottom-right corner\n",
        "        self.grid[self.goal_position] = 1  # Set goal cell value to 1\n",
        "\n",
        "        # Add obstacles to the grid\n",
        "        for obstacle in obstacles:\n",
        "            self.grid[obstacle] = -1\n",
        "\n",
        "        # If random_obstacles is True, randomly place obstacles\n",
        "        if random_obstacles:\n",
        "            num_obstacles = size // 2  # Approximately half of the grid size\n",
        "            for _ in range(num_obstacles):\n",
        "                obstacle_position = (random.randint(0, size-1), random.randint(0, size-1))\n",
        "                # Ensure obstacles are not placed on the starting or goal positions\n",
        "                while obstacle_position == self.agent_position or obstacle_position == self.goal_position:\n",
        "                    obstacle_position = (random.randint(0, size-1), random.randint(0, size-1))\n",
        "                self.grid[obstacle_position] = -1\n",
        "\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Right, Left, Down, Up\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_position = (0, 0)  # Reset agent position to top-left corner\n",
        "\n",
        "    def step(self, action):\n",
        "        # Move the agent according to the selected action\n",
        "        new_position = (self.agent_position[0] + action[0], self.agent_position[1] + action[1])\n",
        "\n",
        "        # Check if new position is within the grid boundaries and not an obstacle\n",
        "        if 0 <= new_position[0] < self.size and 0 <= new_position[1] < self.size and self.grid[new_position] != -1:\n",
        "            self.agent_position = new_position\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = -1  # Default reward for each step\n",
        "        if self.agent_position == self.goal_position:\n",
        "            reward = 10  # Reward for reaching the goal\n",
        "\n",
        "        # Return the new state, reward, and whether the episode is done\n",
        "        return self.agent_position, reward, self.agent_position == self.goal_position\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, num_actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.num_actions = num_actions\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Epsilon-greedy exploration rate\n",
        "        self.q_table = {}\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.num_actions)  # Random action (exploration)\n",
        "        else:\n",
        "            if state not in self.q_table:\n",
        "                self.q_table[state] = np.zeros(self.num_actions)\n",
        "            return np.argmax(self.q_table[state])  # Greedy action (exploitation)\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = np.zeros(self.num_actions)\n",
        "        if next_state not in self.q_table:\n",
        "            self.q_table[next_state] = np.zeros(self.num_actions)\n",
        "        # Q-learning update rule\n",
        "        self.q_table[state][action] += self.alpha * (reward + self.gamma * np.max(self.q_table[next_state]) - self.q_table[state][action])\n",
        "\n",
        "# Function to visualize the grid\n",
        "def visualize_grid(grid, agent_position):\n",
        "    size = grid.shape[0]\n",
        "    for i in range(size):\n",
        "        for j in range(size):\n",
        "            if (i, j) == agent_position:\n",
        "                print(\"A\", end=\" \")  # Agent\n",
        "            elif grid[i, j] == -1:\n",
        "                print(\"X\", end=\" \")  # Obstacle\n",
        "            elif grid[i, j] == 1:\n",
        "                print(\"G\", end=\" \")  # Goal\n",
        "            else:\n",
        "                print(\"_\", end=\" \")  # Empty cell\n",
        "        print()\n",
        "\n",
        "# Initialize grid world environment and Q-learning agent\n",
        "grid_world = GridWorld(size=5, obstacles=[(1, 2), (2, 2)])\n",
        "agent = QLearningAgent(num_actions=4)\n",
        "\n",
        "# Train the agent\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = (0, 0)  # Reset to initial state\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, done = grid_world.step(grid_world.actions[action])\n",
        "        agent.update_q_value(state, action, reward, next_state)\n",
        "        state = next_state\n",
        "    grid_world.reset()\n",
        "\n",
        "# Test the trained agent\n",
        "state = (0, 0)  # Reset to initial state\n",
        "done = False\n",
        "total_reward = 0\n",
        "path = [state]  # Store the path\n",
        "while not done:\n",
        "    visualize_grid(grid_world.grid, state)\n",
        "    action = agent.get_action(state)\n",
        "    next_state, reward, done = grid_world.step(grid_world.actions[action])\n",
        "    total_reward += reward\n",
        "    state = next_state\n",
        "    path.append(state)  # Append next state to path\n",
        "    print(\"Agent's next move:\", next_state)\n",
        "    print(\"Q-values for current state:\")\n",
        "    print(agent.q_table[state])\n",
        "\n",
        "print(\"Reached the goal!\")\n",
        "print(\"Total reward earned:\", total_reward)\n",
        "print(\"Path taken by the agent:\", path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyLP51OEoHCo",
        "outputId": "fd9511d8-0b70-4842-abc3-fa8d86957b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A _ _ _ _ \n",
            "_ _ X _ _ \n",
            "_ _ X _ _ \n",
            "_ _ _ _ _ \n",
            "_ _ _ _ G \n",
            "Agent's next move: (1, 0)\n",
            "Q-values for current state:\n",
            "[ 0.27193333 -0.63150684  0.62882    -1.42606648]\n",
            "_ _ _ _ _ \n",
            "A _ X _ _ \n",
            "_ _ X _ _ \n",
            "_ _ _ _ _ \n",
            "_ _ _ _ G \n",
            "Agent's next move: (2, 0)\n",
            "Q-values for current state:\n",
            "[ 1.8098      0.45183911  1.4210627  -0.50920787]\n",
            "_ _ _ _ _ \n",
            "_ _ X _ _ \n",
            "A _ X _ _ \n",
            "_ _ _ _ _ \n",
            "_ _ _ _ G \n",
            "Agent's next move: (2, 1)\n",
            "Q-values for current state:\n",
            "[1.45511106 0.37326079 3.122      0.2865396 ]\n",
            "_ _ _ _ _ \n",
            "_ _ X _ _ \n",
            "_ A X _ _ \n",
            "_ _ _ _ _ \n",
            "_ _ _ _ G \n",
            "Agent's next move: (3, 1)\n",
            "Q-values for current state:\n",
            "[3.19327455 1.37417815 4.58       1.70478654]\n",
            "_ _ _ _ _ \n",
            "_ _ X _ _ \n",
            "_ _ X _ _ \n",
            "_ A _ _ _ \n",
            "_ _ _ _ G \n",
            "Agent's next move: (4, 1)\n",
            "Q-values for current state:\n",
            "[6.2        1.87706048 4.33508352 2.62565441]\n",
            "_ _ _ _ _ \n",
            "_ _ X _ _ \n",
            "_ _ X _ _ \n",
            "_ _ _ _ _ \n",
            "_ A _ _ G \n",
            "Agent's next move: (4, 2)\n",
            "Q-values for current state:\n",
            "[8.         3.69550156 5.62550638 3.43592027]\n",
            "_ _ _ _ _ \n",
            "_ _ X _ _ \n",
            "_ _ X _ _ \n",
            "_ _ _ _ _ \n",
            "_ _ A _ G \n",
            "Agent's next move: (4, 3)\n",
            "Q-values for current state:\n",
            "[10.          5.69062892  7.62302131  4.94282213]\n",
            "_ _ _ _ _ \n",
            "_ _ X _ _ \n",
            "_ _ X _ _ \n",
            "_ _ _ _ _ \n",
            "_ _ _ A G \n",
            "Agent's next move: (4, 4)\n",
            "Q-values for current state:\n",
            "[0. 0. 0. 0.]\n",
            "Reached the goal!\n",
            "Total reward earned: 3\n",
            "Path taken by the agent: [(0, 0), (1, 0), (2, 0), (2, 1), (3, 1), (4, 1), (4, 2), (4, 3), (4, 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mYhSJ80Fopun"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}